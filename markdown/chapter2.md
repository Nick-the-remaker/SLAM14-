# 初识SLAM
## SLAM框架
### 1.传感器获取信息
+ 单目相机：成本低，但是**无法得知环境尺度**（就像人闭上一只眼睛就失去了环境的立体感）  <br>
单目相机移动后对远处和近处的物体产生的**视差**(Disparity)可以获得深度，后续的SLAM初始化正是依赖单目相机的移动。
+ 双目相机：双目相机由两个摄像头组成，像人眼一样，两个摄像头之间的距离（基线）是已知的。两个相机所形成的视差巧妙地赋予了双目相机感知环境深度的能力。
+ 深度相机：又称RGBD相机，其中的D代表depth深度，即深度相机既可获得彩色画面，也可获得由深度构成的深度图，区别于双目相机的stereo模式，深度相机的原理可分为结构光（Structured-light）和TOF（time of flight）两种。
> 什么是结构光：通过近红外激光器，投射出具有一定结构特征的激光散斑，这样的散斑照射到物体上会因物体形状产生相应的畸变，然后运算单元会将这种结构的变化换算成深度信息。
> 
> TOF呢：就是测量光的飞行时间，像激光测距一样来计算距离，通过探测光脉冲的飞行往返时间来获取深度信息。
### 2.视觉里程计
+ Visual Odometry,简称VO，是一种用计算机视觉的方式来推算出相机走的路程。
  <br>人通过行走时眼前场景的变化可以轻松获得自身的姿态，是正在往前走还是在转弯。只有一个摄像头的小电脑也只能效仿这样的方式，计算机通过获取当前图像帧和前几次出现的场景进行匹配，可以明白自身正在发生怎样的运动（比如图像正在倾斜，说明摄像机本身在旋转），一直累积起来就构成了里程计。
### 3.后端优化
+ 当摄像头工作在较暗的环境下，会影响图像之间的匹配，就像人在大雾中迷失了方向，诸如此类会影响SLAM系统精准工作的因素还有非常非常多，所以后端一直进行优化是很有必要的，即利用滤波器或非线性优化，将运动主体自身和周围空间环境不确定性表达出来并加以修正。
### 4.回环检测
+ 再好的传感器也会有误差，误差在每一帧中进行累积，相机估计的自身位置就会随时间漂移，最后建成的地图会惨不忍睹。但如果相机走着走着着回到了自己曾经去过的地方，它就会发现自己已经远远偏离了初心，回不到过去的路了，此时回环检测机制会将位置估计值拉回来，可以显著消除累积误差，让定位和建图更精准。
### 5.建图
+ SLAM系统全称就是 *Simultaneous Localization and Mapping*，意为实时定位和建图，所以建图也是不可或缺的一部分，相机在运动过程中通过特征点和关键帧构建出二维或三维地图，稀疏或稠密点云地图等等，用于机器人的导航，避障或场景重建等。<br>
<img src="/插图/chapter2/建图.png" width = "576" height = "324" alt="图为orb_slam2建图部分" align=center />


## SLAM问题的数学表述

+ 系统在运行中，其主要作用的可以抽象为两个行为，分别是运动和观测，书中利用两个简明扼要的方程概括了这两个行为<br>
![](/插图/chapter2/方程.png)
+ 在k-1时刻到k时刻，第一个方程主要描述了：机器人通过读取传感器读数，例如IMU，编码盘等来估计当前运动，对应SLAM系统中的定位部分；同理，第二个方程主要描述传感器观测量，如深度相机，激光雷达获取周围环境的路标点(land mark,特征点，关键帧等)来估计外部环境的构建，对应SLAM系统的建图部分。
